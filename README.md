# F1 Strategy AI - QA Framework
Testing AI robustness during F1's 2026 regulation revolution. 

## The Challenge
The 2026 F1 season introduces the biggest regulation changes since 2014: 
- New power units (50/50 electric/combustion split)
- Active aerodynamcis
- Lighter cars (30kg reduction)
- Redesigned tire compounds
- 100% sustainable fuel

**This project tracks how an AI trained on 2023-2025 data adapts to fundamentally different 2026 regulations.**

## Current Status: v0.3.0 - Production Ready

**Test Coverage: 100% (27/27 tests passing)**
```
Unit Tests: 14/14
Evaluation Tests: 5/5
Adversarial Tests: 8/8
-----------------------
Total: 27/27
```

**Code Coverage: 87%**
```
agent.py:            89% coverage
input_validator.py:  97% coverage  
schema_validator.py: 71% coverage
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:               87% coverage
```
![Coverage](coverage.svg)


**Development** (Feb 2026)
- [x] FastF1 integration
- [x] Historical validation (2023-2025)
- [x] Core rule-based agent with guardrails
- [x] Adversarial testing and security validation
- [ ] Additional track scenarios
- [ ] RAG implementation
- [ ] Bahrain GP 2026 ready (March 2)

## Key Achievements
### Agent Performance
- **100% accuracy** on Silverstone 2023 golden dataset (5/5 scenarios)
- **Production-grade validation** with FIA tire regulation compliance
- **Safety guardrails** for weather/tire compatibility detection
- **Confidence calibration** (0.65-0.98 range for appropriate uncertainty)

### Security & Robustness

| Metric | Before (v0.2.0) | After (v0.3.0) |
|--------|-----------------|----------------|
| **Test Pass Rate** | 25% (2/8) | 100% (8/8) | 
| **Hallucination Risk** | CRITICAL | ELIMINATED |
| **Data Integrity** | NONE | COMPREHENSIVE |
| **Safety Guardrails** | NONE | PRODUCTION-READY |

### Adversarial Testing Results
Adversarial test suite validates the agent against edge cases and attack scenarios:

- **Rejects fictional tire compounds** (e.g., HYPERSOFT from pre-2019 era)  
- **Validates tire age bounds** (rejects negative ages, impossibly old tires)  
- **Enforces race lap limits** (detects lap 99 in 80-lap maximum race)  
- **Detects dangerous scenarios** (slicks in heavy rain â†’ BOX with 0.98 confidence)  
- **Prevents tire/weather mismatches** (wet tires on dry track â†’ BOX with 0.95 confidence)  
- **Calibrates confidence appropriately** (0.90 for clear decisions, 0.65 for ambiguous)  

---

## Test Categories
### Unit Tests (14 tests)
Validates golden dataset structure, schema compliance, and data integrity across all race scenarios.

**What it tests:**
- JSON structure validity
- Required fields presence
- FIA regulation compliance (tire compounds, lap bounds)
- Data type validation
- Cross-dataset consistency

### Evaluation Tests (5 tests)
Tests agent against real race scenarios from **Silverstone 2023 British Grand Prix**:

1. **Norris Lap 1** - Fresh tire handling at race start (STAY_OUT, 0.90 confidence)
2. **Russell Lap 28** - Extended soft stint beyond expected degradation (STAY_OUT, 0.65 confidence)
3. **Piastri Lap 30** - Pre-safety car pit timing decision (BOX, 0.85 confidence)
4. **Norris Lap 34** - Cold hard tire restart defense (STAY_OUT, 0.90 confidence)
5. **Hamilton Lap 34** - Safety car opportunism for tire advantage (BOX, 0.85 confidence)

**Result:** 5/5 correct predictions (100% accuracy)

### Adversarial Tests (8 tests)
Exposes vulnerabilities and validates guardrails against:

- **Invalid compound detection** (fictional tire types)
- **Data integrity validation** (negative/impossible values)
- **Safety-critical scenario handling** (dangerous weather/tire combinations)
- **Confidence calibration** (appropriate uncertainty quantification)
- **Edge case robustness** (boundary conditions and race limits)

**Result:** 8/8 passing (0% vulnerability rate)

---

## Quick Start

### Installation
```bash
# Clone the repository
git clone 
cd f1-strategy-ai-qa-framework

# Create virtual environment
python3.10 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

### Run Tests
```bash
# Run all tests
pytest tests/ -v

# Run specific test suites
pytest tests/unit/ -v           # Unit tests (14 tests)
pytest tests/evaluation/ -v     # Golden dataset validation (5 tests)
pytest tests/adversarial/ -v    # Security & edge cases (8 tests)

### Explore Data
```bash
# Explore FastF1 data for Silverstone 2023
python explore_fastf1.py
```

---

## Project Structure
```
f1-strategy-ai-qa-framework/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pytest.ini                    # Test configuration
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cache/                        # FastF1 cached race data
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ golden/                       # Real race scenarios from actual GPs
â”‚   â”‚   â””â”€â”€ silverstone_2023_scenarios.json
â”‚   â””â”€â”€ adversarial/                  # Edge cases & attack scenarios
â”‚       â””â”€â”€ impossible_scenarios.json
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ agent.py                  # F1StrategyAgent - core decision engine
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ validators/
â”‚       â”œâ”€â”€ input_validator.py        # Input validation & guardrails
â”‚       â”œâ”€â”€ schema_validator.py       # JSON schema validation
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py                   # Shared pytest fixtures
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â””â”€â”€ test_golden_dataset.py    # Dataset structure validation
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ test_strategy_agent.py    # Agent performance tests
â”‚   â””â”€â”€ adversarial/
â”‚       â””â”€â”€ test_hallucination_guards.py  # Security & robustness tests
â”‚
â”œâ”€â”€ explore_fastf1.py                 # Data exploration script
â”œâ”€â”€ requirements.txt                  # Python dependencies
â””â”€â”€ README.md                         # This file
```

---

## Tech Stack

**Data & Analysis:**
- **FastF1** (â‰¥3.0.0) - F1 race telemetry and timing data
- **pandas** (â‰¥2.0.0) - Data manipulation and analysis

**Testing & Quality:**
- **pytest** (â‰¥7.0.0) - Test framework
- **pytest-bdd** (â‰¥7.0.0) - Behavior-driven development (future E2E tests)

**Development:**
- **Python 3.10+** - Language runtime
- **python-dotenv** (â‰¥1.0.0) - Environment configuration

---

## Project Goals
### Core Objectives

1. **Build RAG-based F1 strategy AI** - Rule-based agent v0.3.0 complete
2. **Validate on 2023-2025 historical races** - Silverstone 2023 golden dataset
3. **Deploy for 2026 season predictions** - Ready for Bahrain GP (March 2, 2026)
4. **Track adaptation across 24 races** - Live season tracking framework
5. **Measure learning curve during regulation shift** - Quantify AI adaptation

### Quality Engineering Goals
- Demonstrate AI/ML testing methodologies
- Implement production-grade validation layers
- Build comprehensive test pyramid (unit â†’ integration â†’ adversarial)
- Quantify security improvements (25% â†’ 100% test pass rate)
- Track real-world AI performance across 2026 season

---

## Methodology & Learnings

### Test-Driven Development Approach

**Phase 1: Red (Baseline)**
- Created adversarial tests that exposed vulnerabilities
- Result: 2/8 passing (25% success rate)
- Identified critical gaps: no validation, no safety guardrails

**Phase 2: Green (Implementation)**
- Built input validation layer with FIA compliance
- Added weather/tire compatibility detection
- Result: 7/8 passing (87.5% success rate)

**Phase 3: Refactor (Optimization)**
- Tuned validation bounds (MAX_RACE_LAPS: 100 â†’ 80)
- Fixed edge cases and confidence calibration
- Result: 8/8 passing (100% success rate)

### Key Insights

1. **Guardrails are essential** - Rule-based systems need strict input validation
2. **Confidence calibration matters** - 0.65 for ambiguous vs 0.98 for safety-critical
3. **Adversarial testing works** - Exposed vulnerabilities that unit tests missed
4. **Domain expertise is crucial** - F1 knowledge informed realistic test scenarios

---

## Next Steps

### Short Term (Before Bahrain GP - March 2, 2026)
- [ ] Add golden datasets for 4 more tracks (Spa, Singapore, Australia, SÃ£o Paulo)
- [ ] Implement RAG retrieval layer with vector database (Chroma/FAISS)
- [ ] Add confidence tracking and accuracy metrics
- [ ] Build simple HTML dashboard for test results

### Medium Term (2026 Season)
- [ ] Deploy prediction system for Bahrain GP 2026
- [ ] Track accuracy across first 5 races (March-April)
- [ ] Measure AI adaptation to new regulations
- [ ] Document learning curve (expected: 58% â†’ 78% accuracy over 5 races)

### Long Term (Season End)
- [ ] Full season analysis (24 races)
- [ ] Compare AI predictions vs actual team strategies
- [ ] Publish findings on regulation adaptation
- [ ] Identify regulation-invariant strategic principles

---

## 2026 Season Tracking

**Starting March 2, 2026**, this framework will track live predictions:
```
Race 1: Bahrain      - TBD
Race 2: Saudi Arabia - TBD
Race 3: Australia    - TBD
...
```

Each race will update with:
- Pre-race predictions
- Actual outcomes
- Accuracy metrics
- Lessons learned

**Goal:** Measure how quickly AI adapts to 2026 regulation changes.

---

## ðŸ‘¤ Author
**Ernest** - Senior Lead QA Engineer & Technical Product Owner